{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e514ea06",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
    "Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36884fa8",
   "metadata": {},
   "source": [
    "### Notes\n",
    "NLTK (Natural Language Toolkit) is the go-to API for NLP (Natural Language Processing) with Python. \n",
    "It is a really powerful tool to preprocess text data for further analysis like with ML models for instance. \n",
    "It helps convert text into numbers, which the model can then easily work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735fee9",
   "metadata": {},
   "source": [
    "### 1.Tokenization\n",
    "One of the very basic things we want to do is dividing a body of text into words or sentences. This is called tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e980d5",
   "metadata": {},
   "source": [
    "### 2.Stop-words\n",
    "Stop-words are basically words that don’t have strong meaningful connotations for instance, ‘and’, ‘a’, ‘it's’, ‘they’, etc. These have a meaningful impact when we use them to communicate with each other but for analysis by a computer, they are not really that useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cc942",
   "metadata": {},
   "source": [
    "### 3.Stemming /VS/ Lemmatization\n",
    "- 1.Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "--> Stemming : the word ‘Caring‘ would return ‘Car‘.\n",
    "- 2.Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
    "--> Lemmatizing the word ‘Caring‘ would return ‘Care‘.\n",
    "Stemming is used in case of large dataset where performance is an issue.becuase Lemmatization is computationally expensive.\n",
    "- The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead, it uses lexical knowledge bases to get the correct base forms of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8263da3",
   "metadata": {},
   "source": [
    "### 4. POS tagging : Part of Speech Tagging\n",
    "- Part of Speech Tagging (POS-Tag) is the labeling of the words in a text according to \n",
    "their word types (noun, adjective, adverb, verb, etc.)\n",
    "- The pos_tag() method takes in a list of tokenized words, and tags each of them with a corresponding Parts of Speech identifier into tuples. \n",
    "- For example, \n",
    "    - VB refers to ‘verb’, \n",
    "    - NNS refers to ‘plural nouns’, \n",
    "    - DT refers to a ‘determiner’. \n",
    "    - Noun (N)- Daniel, London, table, dog, teacher, pen, city, happiness, hope\n",
    "    - Verb (V)- go, speak, run, eat, play, live, walk, have, like, are, is\n",
    "    - Adjective(ADJ)- big, happy, green, young, fun, crazy, three\n",
    "    - Adverb(ADV)- slowly, quietly, very, always, never, too, well, tomorrow\n",
    "    - Preposition (P)- at, on, in, from, with, near, between, about, under\n",
    "    - Conjunction (CON)- and, or, but, because, so, yet, unless, since, if\n",
    "    - Pronoun(PRO)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    "    - Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c21925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40205c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "914bc254",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc716671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40866a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer:  ['My', 'name', 'is', 'kaveri', 'vinod', 'raut', '.', 'I', 'am', 'computers', 'engineering', 'students', 'studing', 'at', 'PICT', 'pune', 'under', 'SPPU', '.', 'I', 'likes', 'playing', 'cricket', '.', 'PICT', 'is', 'top', 'institute', 'in', 'pune', 'under', 'SPPU', '.', 'I', 'am', 'having', 'caring', 'nature', '.', 'I', 'think', 'guitar', 'is', 'interesting', 'instrument', '.']\n",
      "Sent Tokenizer:  ['My name is Krushna vinod raut.', 'I am caring IT engineer working in Citi Bank.']\n"
     ]
    }
   ],
   "source": [
    "string1 = \"My name is kaveri vinod raut. I am computers engineering students studing at PICT pune under SPPU. I likes playing cricket. PICT is top institute in pune under SPPU. I am having caring nature. I think guitar is  interesting instrument.\"\n",
    "token_string1 = nltk.word_tokenize(string1)\n",
    "print(\"Word Tokenizer: \",token_string1)\n",
    "\n",
    "string2 = \"My name is Krushna vinod raut. I am caring IT engineer working in Citi Bank.\"\n",
    "token_string2 = nltk.sent_tokenize(string2)\n",
    "print(\"Sent Tokenizer: \",token_string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719883d9",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e11b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6eceb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial String :\n",
      " My name is kaveri vinod raut . I am computers engineering students studing at PICT pune under SPPU . I likes playing cricket . PICT is top institute in pune under SPPU . I am having caring nature . I think guitar is interesting instrument .  \n",
      "\n",
      "after removal of stopword:\n",
      " My name kaveri vinod raut . I computers engineering students studing PICT pune SPPU . I likes playing cricket . PICT top institute pune SPPU . I caring nature . I think guitar interesting instrument .  \n",
      "\n",
      "Stop words removed are:  ['is', 'am', 'at', 'under', 'is', 'in', 'under', 'am', 'having', 'is']\n"
     ]
    }
   ],
   "source": [
    "# collect all the stop words from 'english' language\n",
    "all_stopwords = stopwords.words('english')\n",
    "# print(all_stopwords)\n",
    "\n",
    "# now we have to delete all the stop words from our tokenized token_string1 => firstly string must be tokenized \n",
    "# store remaining words in new removed_Stop_words_string1 into none_stopword_string1[]\n",
    "nonstop_string1 = \"\"\n",
    "with_stopword_string = \"\"\n",
    "stopword_found = []\n",
    "\n",
    "for word in token_string1:  #iterate on tokenized words\n",
    "    with_stopword_string += word\n",
    "    with_stopword_string += \" \"\n",
    "    if word not in all_stopwords: #if word do not belong to stopword => add it to nonstop_string1[] array\n",
    "        nonstop_string1 += word\n",
    "        nonstop_string1 += \" \"\n",
    "    if word in all_stopwords:     #if word belong to stopword => add it to stopword_found[] array\n",
    "        stopword_found.append(word)\n",
    "        \n",
    "# printing string formate of words after of tokenized words array with the stopwords\n",
    "print(\"initial String :\\n\",with_stopword_string,\"\\n\")\n",
    "\n",
    "# printing new string of words after deleting the stopwords \n",
    "print(\"after removal of stopword:\\n\",nonstop_string1,\"\\n\")\n",
    "\n",
    "#printing the stopwords found in our token_string1\n",
    "print(\"Stop words removed are: \",stopword_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a543d",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8509e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves ->  leav\n",
      "engineering ->  engin\n",
      "beeches ->  beech\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  \n",
    "\n",
    "ps = PorterStemmer()  #Initialize Python porter stemmer\n",
    "word = \"leaves\"\n",
    "print(\"leaves -> \",ps.stem(word))\n",
    "print(\"engineering -> \",ps.stem(\"engineering\"))\n",
    "print(\"beeches -> \",ps.stem(\"beeches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77908ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My                  my                  \n",
      "name                name                \n",
      "kaveri              kaveri              \n",
      "vinod               vinod               \n",
      "raut                raut                \n",
      ".                   .                   \n",
      "I                   i                   \n",
      "computers           comput              \n",
      "engineering         engin               \n",
      "students            student             \n",
      "studing             stude               \n",
      "PICT                pict                \n",
      "pune                pune                \n",
      "SPPU                sppu                \n",
      ".                   .                   \n",
      "I                   i                   \n",
      "likes               like                \n",
      "playing             play                \n",
      "cricket             cricket             \n",
      ".                   .                   \n",
      "PICT                pict                \n",
      "top                 top                 \n",
      "institute           institut            \n",
      "pune                pune                \n",
      "SPPU                sppu                \n",
      ".                   .                   \n",
      "I                   i                   \n",
      "caring              care                \n",
      "nature              natur               \n",
      ".                   .                   \n",
      "I                   i                   \n",
      "think               think               \n",
      "guitar              guitar              \n",
      "interesting         interest            \n",
      "instrument          instrument          \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "#firstly need to tokenize the string into words then pass words for the stemming\n",
    "\n",
    "# using tokenized string got after deleting stopwords ===> nonstop_string1\n",
    "#print(nonstop_string1)\n",
    "\n",
    "split_str = nonstop_string1.split()\n",
    "\n",
    "for word in split_str:\n",
    "    #print(word)\n",
    "    #print(word,\" -> \", ps.stem(word))\n",
    "    print(\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962a141",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5d853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63d557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves ->  leaf\n",
      "workers ->  worker\n",
      "beeches ->  beech\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "word = \"leaves\"\n",
    "print(\"leaves -> \",wnl.lemmatize(\"leaves\"))\n",
    "print(\"workers -> \",wnl.lemmatize(\"workers\"))\n",
    "print(\"beeches -> \",wnl.lemmatize(\"beeches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0da7f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name kaveri vinod raut . I computers engineering students studing PICT pune SPPU . I likes playing cricket . PICT top institute pune SPPU . I caring nature . I think guitar interesting instrument . \n",
      "My                  My                  \n",
      "name                name                \n",
      "kaveri              kaveri              \n",
      "vinod               vinod               \n",
      "raut                raut                \n",
      ".                   .                   \n",
      "I                   I                   \n",
      "computers           computers           \n",
      "engineering         engineer            \n",
      "students            students            \n",
      "studing             stud                \n",
      "PICT                PICT                \n",
      "pune                pune                \n",
      "SPPU                SPPU                \n",
      ".                   .                   \n",
      "I                   I                   \n",
      "likes               like                \n",
      "playing             play                \n",
      "cricket             cricket             \n",
      ".                   .                   \n",
      "PICT                PICT                \n",
      "top                 top                 \n",
      "institute           institute           \n",
      "pune                pune                \n",
      "SPPU                SPPU                \n",
      ".                   .                   \n",
      "I                   I                   \n",
      "caring              care                \n",
      "nature              nature              \n",
      ".                   .                   \n",
      "I                   I                   \n",
      "think               think               \n",
      "guitar              guitar              \n",
      "interesting         interest            \n",
      "instrument          instrument          \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "#storing this more preprocessed string (tokenized + deleted stopword + lemmitized) into final_string\n",
    "final_string_list = []\n",
    "\n",
    "#print(nonstop_string1)\n",
    "split_str = nonstop_string1.split()\n",
    "print(nonstop_string1)\n",
    "\n",
    "for word in split_str:\n",
    "    new_str = wnl.lemmatize(word,pos=\"v\")\n",
    "    print(\"{0:20}{1:20}\".format(word, new_str))\n",
    "    final_string_list.append(new_str) #this final_string_list[] string array for POS tagging purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0888bc",
   "metadata": {},
   "source": [
    "# POS-tagging : Part Of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d651089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ebbc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged words are: \n",
      "\n",
      "('My', 'PRP$') \n",
      "\n",
      "('name', 'NN') \n",
      "\n",
      "('kaveri', 'VB') \n",
      "\n",
      "('vinod', 'NN') \n",
      "\n",
      "('raut', 'NN') \n",
      "\n",
      "('.', '.') \n",
      "\n",
      "('I', 'PRP') \n",
      "\n",
      "('computers', 'NNS') \n",
      "\n",
      "('engineer', 'VBP') \n",
      "\n",
      "('students', 'NNS') \n",
      "\n",
      "('stud', 'JJ') \n",
      "\n",
      "('PICT', 'NNP') \n",
      "\n",
      "('pune', 'NN') \n",
      "\n",
      "('SPPU', 'NNP') \n",
      "\n",
      "('.', '.') \n",
      "\n",
      "('I', 'PRP') \n",
      "\n",
      "('like', 'VBP') \n",
      "\n",
      "('play', 'NN') \n",
      "\n",
      "('cricket', 'NN') \n",
      "\n",
      "('.', '.') \n",
      "\n",
      "('PICT', 'NNP') \n",
      "\n",
      "('top', 'JJ') \n",
      "\n",
      "('institute', 'NN') \n",
      "\n",
      "('pune', 'NN') \n",
      "\n",
      "('SPPU', 'NNP') \n",
      "\n",
      "('.', '.') \n",
      "\n",
      "('I', 'PRP') \n",
      "\n",
      "('care', 'VBP') \n",
      "\n",
      "('nature', 'JJ') \n",
      "\n",
      "('.', '.') \n",
      "\n",
      "('I', 'PRP') \n",
      "\n",
      "('think', 'VBP') \n",
      "\n",
      "('guitar', 'JJ') \n",
      "\n",
      "('interest', 'NN') \n",
      "\n",
      "('instrument', 'NN') \n",
      "\n",
      "('.', '.') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# required tokenized string to assign the POS tags\n",
    "# this final_string_list[] string array for POS tagging purpose\n",
    "pos_tag_string = pos_tag(final_string_list)\n",
    "print(\"POS Tagged words are: \\n\")\n",
    "for pos_word in pos_tag_string:\n",
    "    print(pos_word,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353e8e7",
   "metadata": {},
   "source": [
    "# Term Frequency (TF) \n",
    "- count of terms/words in document (single doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe4ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to combine all words into single set, using set to store all unique words together \n",
    "word_set = set()\n",
    "# using this final_string_list[] prepared after lemmitization\n",
    "word_set = word_set.union(set(final_string_list)) #deplicate words removal \n",
    "df = dict.fromkeys(word_set, 0) #lets add a way to count the words using a dictionary key-value pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e356a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in final_string_list:\n",
    "    df[word] = df[word]+1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44590410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stud': 1,\n",
       " 'guitar': 1,\n",
       " 'vinod': 1,\n",
       " 'care': 1,\n",
       " 'kaveri': 1,\n",
       " 'PICT': 2,\n",
       " 'nature': 1,\n",
       " 'raut': 1,\n",
       " 'play': 1,\n",
       " 'instrument': 1,\n",
       " 'pune': 2,\n",
       " 'computers': 1,\n",
       " 'SPPU': 2,\n",
       " 'top': 1,\n",
       " 'interest': 1,\n",
       " 'cricket': 1,\n",
       " 'engineer': 1,\n",
       " 'think': 1,\n",
       " 'name': 1,\n",
       " '.': 6,\n",
       " 'students': 1,\n",
       " 'like': 1,\n",
       " 'institute': 1,\n",
       " 'I': 4,\n",
       " 'My': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fb406",
   "metadata": {},
   "source": [
    "## tf(t,d) = count of word in document / number of all words in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e539bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stud': 0.0388316669075566,\n",
       " 'guitar': 0.0388316669075566,\n",
       " 'vinod': 0.0388316669075566,\n",
       " 'care': 0.0388316669075566,\n",
       " 'kaveri': 0.0388316669075566,\n",
       " 'PICT': 0.030469722583557124,\n",
       " 'nature': 0.0388316669075566,\n",
       " 'raut': 0.0388316669075566,\n",
       " 'play': 0.0388316669075566,\n",
       " 'instrument': 0.0388316669075566,\n",
       " 'pune': 0.030469722583557124,\n",
       " 'computers': 0.0388316669075566,\n",
       " 'SPPU': 0.030469722583557124,\n",
       " 'top': 0.0388316669075566,\n",
       " 'interest': 0.0388316669075566,\n",
       " 'cricket': 0.0388316669075566,\n",
       " 'engineer': 0.0388316669075566,\n",
       " 'think': 0.0388316669075566,\n",
       " 'name': 0.0388316669075566,\n",
       " '.': 0.017216354396899832,\n",
       " 'students': 0.0388316669075566,\n",
       " 'like': 0.0388316669075566,\n",
       " 'institute': 0.0388316669075566,\n",
       " 'I': 0.022107778259557644,\n",
       " 'My': 0.0388316669075566}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF = {} #tuple of tf values for each word\n",
    "corpusCount = len(final_string_list)\n",
    "for word, count in df.items():  # iterate over the dictionary key,value pair\n",
    "    TF[word] = count / corpusCount\n",
    "    \n",
    "TF # print tf-values ka tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249cfe1",
   "metadata": {},
   "source": [
    "# Inverse document Frequency (IDF) \n",
    "- count of documents in which word appear\n",
    "- idf(t) = total count of document set/occurrence of t in documents\n",
    "- IDF = N / df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "232e6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "IDF_dict = {} #tuple of tf values for each word\n",
    "N = len(df) #count of corpus i.e. total count of document set\n",
    "IDF_dict = df\n",
    "for word, val in IDF_dict.items(): #iterate over the dictionary key,value pair\n",
    "    IDF_dict[word] = math.log10(N / val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba9e2aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stud': 1.2524514742164006,\n",
       " 'guitar': 1.2524514742164006,\n",
       " 'vinod': 1.2524514742164006,\n",
       " 'care': 1.2524514742164006,\n",
       " 'kaveri': 1.2524514742164006,\n",
       " 'PICT': 1.357769007767367,\n",
       " 'nature': 1.2524514742164006,\n",
       " 'raut': 1.2524514742164006,\n",
       " 'play': 1.2524514742164006,\n",
       " 'instrument': 1.2524514742164006,\n",
       " 'pune': 1.357769007767367,\n",
       " 'computers': 1.2524514742164006,\n",
       " 'SPPU': 1.357769007767367,\n",
       " 'top': 1.2524514742164006,\n",
       " 'interest': 1.2524514742164006,\n",
       " 'cricket': 1.2524514742164006,\n",
       " 'engineer': 1.2524514742164006,\n",
       " 'think': 1.2524514742164006,\n",
       " 'name': 1.2524514742164006,\n",
       " '.': 1.6056963139188123,\n",
       " 'students': 1.2524514742164006,\n",
       " 'like': 1.2524514742164006,\n",
       " 'institute': 1.2524514742164006,\n",
       " 'I': 1.4970924079355536,\n",
       " 'My': 1.2524514742164006}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2f8a0",
   "metadata": {},
   "source": [
    "# TF-IDF \n",
    "## tf-idf(t, d) = tf(t, d) * log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d510a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = {}\n",
    "for word, val in TF.items():\n",
    "    TF_IDF[word] = val * IDF_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33fbbec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stud': 0.04863477846464948,\n",
       " 'guitar': 0.04863477846464948,\n",
       " 'vinod': 0.04863477846464948,\n",
       " 'care': 0.04863477846464948,\n",
       " 'kaveri': 0.04863477846464948,\n",
       " 'PICT': 0.04137084499922329,\n",
       " 'nature': 0.04863477846464948,\n",
       " 'raut': 0.04863477846464948,\n",
       " 'play': 0.04863477846464948,\n",
       " 'instrument': 0.04863477846464948,\n",
       " 'pune': 0.04137084499922329,\n",
       " 'computers': 0.04863477846464948,\n",
       " 'SPPU': 0.04137084499922329,\n",
       " 'top': 0.04863477846464948,\n",
       " 'interest': 0.04863477846464948,\n",
       " 'cricket': 0.04863477846464948,\n",
       " 'engineer': 0.04863477846464948,\n",
       " 'think': 0.04863477846464948,\n",
       " 'name': 0.04863477846464948,\n",
       " '.': 0.027644236794221996,\n",
       " 'students': 0.04863477846464948,\n",
       " 'like': 0.04863477846464948,\n",
       " 'institute': 0.04863477846464948,\n",
       " 'I': 0.033097386988706436,\n",
       " 'My': 0.04863477846464948}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beadd5b",
   "metadata": {},
   "source": [
    "## Result : tf-idf now is a the right measure to evaluate how important a word is to a document in a collection or corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
